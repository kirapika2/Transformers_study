{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7xOV8ePyCzkk58KieDGpN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.57.6"
      ],
      "metadata": {
        "id": "kaRSHDObEQ48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLuNsNMgk64J"
      },
      "outputs": [],
      "source": [
        "# 必要なもの\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
        "summaries = {}\n",
        "\n",
        "## NLTK パッケージ\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "## ベースライン\n",
        "def three_sentence_summary(text):\n",
        "  return \"\\n\".join(sent_tokenize(text)[:3])\n",
        "summaries[\"baseline\"] = three_sentence_summary(sample_text)\n",
        "\n",
        "## GPT-2 による結果\n",
        "from transformers import pipeline, set_seed\n",
        "set_seed(42)\n",
        "pipe = pipeline(\"text-generation\", model=\"gpt2-xl\")\n",
        "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
        "pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
        "summaries[\"gpt2\"] = \"\\n\".join(\n",
        "    sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))\n",
        "\n",
        "## T5 による結果\n",
        "pipe = pipeline(\"summarization\", model=\"t5-large\")\n",
        "pipe_out = pipe(sample_text)\n",
        "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n",
        "\n",
        "## BART による結果\n",
        "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "pipe_out = pipe(sample_text)\n",
        "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n",
        "\n",
        "## PEGASUS による結果\n",
        "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
        "pipe_out = pipe(sample_text)\n",
        "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 結果の比較\n",
        "print(\"GROUND TRUTH\")\n",
        "print(dataset[\"train\"][1][\"highlights\"])\n",
        "print(\"\")\n",
        "\n",
        "for model_name in summaries:\n",
        "  print(model_name.upper())\n",
        "  print(summaries[model_name])\n",
        "  print(\"\")"
      ],
      "metadata": {
        "id": "HCgUgdqnENTc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
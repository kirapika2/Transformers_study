{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO6hCTeDKr+SxhMVnzLRSi3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxdtsttCHBRc"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.57.6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なもの\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
        "summaries = {}\n",
        "\n",
        "## NLTK パッケージ\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "## ベースライン\n",
        "def three_sentence_summary(text):\n",
        "  return \"\\n\".join(sent_tokenize(text)[:3])\n",
        "summaries[\"baseline\"] = three_sentence_summary(sample_text)\n",
        "\n",
        "## GPT-2 による結果\n",
        "from transformers import pipeline, set_seed\n",
        "set_seed(42)\n",
        "pipe = pipeline(\"text-generation\", model=\"gpt2-xl\")\n",
        "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
        "pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
        "summaries[\"gpt2\"] = \"\\n\".join(\n",
        "    sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))\n",
        "\n",
        "## T5 による結果\n",
        "pipe = pipeline(\"summarization\", model=\"t5-large\")\n",
        "pipe_out = pipe(sample_text)\n",
        "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n",
        "\n",
        "## BART による結果\n",
        "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "pipe_out = pipe(sample_text)\n",
        "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n",
        "\n",
        "## PEGASUS による結果\n",
        "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
        "pipe_out = pipe(sample_text)\n",
        "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .\", \".\\n\")"
      ],
      "metadata": {
        "id": "WFRbiDbHOStS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パッケージインスト―ル\n",
        "!pip install evaluate sacrebleu rouge_score"
      ],
      "metadata": {
        "id": "oknF0On7Rc9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU スコアメトリック\n",
        "from sacrebleu.metrics import BLEU\n",
        "\n",
        "bleu_metric = BLEU(smooth_method=\"floor\", smooth_value=0)"
      ],
      "metadata": {
        "id": "OkJZSbW7Odxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# スコア確認\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_bleu_results(bleu_score):\n",
        "  results = {}\n",
        "  results[\"score\"] = bleu_score.score\n",
        "  results[\"counts\"] = bleu_score.counts\n",
        "  results[\"totals\"] = bleu_score.totals\n",
        "  results[\"precisions\"] = [np.round(p, 2) for p in bleu_score.precisions]\n",
        "  results[\"bp\"] = bleu_score.bp\n",
        "  results[\"sys_len\"] = bleu_score.sys_len\n",
        "  results[\"ref_len\"] = bleu_score.ref_len\n",
        "  results[\"ratio\"] = bleu_score.ratio\n",
        "  return results\n",
        "\n",
        "bleu_score = bleu_metric.sentence_score(hypothesis=\"the the the the the the\", references=[\"the cat is on the mat\"])\n",
        "pd.DataFrame.from_dict(get_bleu_results(bleu_score), orient=\"index\", columns=[\"Value\"])"
      ],
      "metadata": {
        "id": "doP7u6ZxPAla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 予測が良い場合のスコア確認\n",
        "bleu_score = bleu_metric.sentence_score(hypothesis=\"the cat is on mat\", references=[\"the cat is on the mat\"])\n",
        "pd.DataFrame.from_dict(get_bleu_results(bleu_score), orient=\"index\", columns=[\"Value\"])"
      ],
      "metadata": {
        "id": "Ke97n4tDQIX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE スコアメトリック\n",
        "import evaluate\n",
        "rouge_metric = evaluate.load('rouge')"
      ],
      "metadata": {
        "id": "H4cScqxdeP_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE スコア計算\n",
        "reference = dataset[\"train\"][1][\"highlights\"]\n",
        "records = []\n",
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "\n",
        "for model_name in summaries:\n",
        "  score = rouge_metric.compute(predictions=[summaries[model_name]], references=[reference])\n",
        "  rouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n",
        "  records.append(rouge_dict)\n",
        "pd.DataFrame.from_records(records, index=summaries.keys())"
      ],
      "metadata": {
        "id": "teWo7BgZfyue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObpPegfNWU2aYY1l7QlAgi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzjE1Qx2LFLM"
      },
      "outputs": [],
      "source": [
        "# トークン分類用カスタムモデル\n",
        "import torch.nn as nn\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel\n",
        "\n",
        "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "  config_class = XLMRobertaConfig\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.num_labels = config.num_labels\n",
        "    # ボディをロード\n",
        "    self.roberta = RobertaModel(config, add_pooling_layer=False) # [CLS]トークンによる表現抽出層の無効化\n",
        "    # トークン分類ヘッドの用意\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "    # 重みのロードと初期化\n",
        "    self.init_weights()\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
        "              labels=None, **kwargs):\n",
        "    # ボディによりエンコーダの表現を取得\n",
        "    outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
        "                           token_type_ids=token_type_ids, **kwargs)\n",
        "    # 分類器を適用\n",
        "    sequence_output = self.dropout(outputs[0]) # 最後の隠れ状態\n",
        "    logits = self.classifier(sequence_output)\n",
        "\n",
        "    # 損失計算\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss()\n",
        "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    # モデルの出力オブジェクトとして返す\n",
        "    return TokenClassifierOutput(loss=loss, logits=logits,\n",
        "                                 hidden_states=outputs.hidden_states,\n",
        "                                 attentions=outputs.attentions)"
      ]
    }
  ]
}
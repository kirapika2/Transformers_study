{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi/xjBiS6x6Q4SmwpG8gMT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0G0wofB1w1W"
      },
      "outputs": [],
      "source": [
        "# ドイツ語の例\n",
        "## 固有表現のデータセット用意\n",
        "from collections import defaultdict\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
        "fracs = [0.629, 0.229, 0.084, 0.059]\n",
        "panx_ch = defaultdict(DatasetDict)\n",
        "\n",
        "for lang, frac in zip(langs,fracs):\n",
        "  ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
        "  for split in ds:\n",
        "    panx_ch[lang][split] = (\n",
        "        ds[split]\n",
        "        .shuffle(seed=0)\n",
        "        .select(range(int(frac*ds[split].num_rows)))\n",
        "    )\n",
        "\n",
        "## ドイツ語の例\n",
        "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
        "def create_tag_names(batch):\n",
        "  return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
        "panx_de = panx_ch[\"de\"].map(create_tag_names)\n",
        "de_example = panx_de[\"train\"][0]\n",
        "\n",
        "words, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]\n",
        "print(f\"Tokens: {words}\") # 各トークン内容\n",
        "print(f\"Labels: {labels}\") # 各トークンラベル"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# トークン化\n",
        "## トークナイザー\n",
        "from transformers import AutoTokenizer\n",
        "xlmr_model_name = \"xlm-roberta-base\"\n",
        "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)\n",
        "\n",
        "## トークン化処理\n",
        "import pandas as pd\n",
        "tokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"]) # ID をトークン化\n",
        "pd.DataFrame([tokens], index=[\"Tokens\"])"
      ],
      "metadata": {
        "id": "NncjGbIS4ZQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2つ目以降のサブワード表現をマスク\n",
        "word_ids = tokenized_input.word_ids()\n",
        "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])"
      ],
      "metadata": {
        "id": "zBtbCC_X518X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習したくないトークンのラベルを -100\n",
        "previous_word_idx = None\n",
        "label_ids = []\n",
        "\n",
        "for word_idx in word_ids:\n",
        "  if word_idx is None or word_idx == previous_word_idx: # 特殊トークンもしくは連続トークン\n",
        "    label_ids.append(-100)\n",
        "  elif word_idx != previous_word_idx:\n",
        "    label_ids.append(labels[word_idx])\n",
        "  previous_word_idx = word_idx\n",
        "\n",
        "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
        "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
        "index = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
        "\n",
        "pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)"
      ],
      "metadata": {
        "id": "PiSTSxDT6xTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセット全体に拡張\n",
        "def tokenize_and_align_labels(examples):\n",
        "  tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\n",
        "                                    is_split_into_words=True)\n",
        "  labels = []\n",
        "  for idx, label in enumerate(examples[\"ner_tags\"]):\n",
        "    word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "    for word_idx in word_ids:\n",
        "      if word_idx is None or word_idx == previous_word_idx:\n",
        "        label_ids.append(-100)\n",
        "      else:\n",
        "        label_ids.append(label[word_idx])\n",
        "      previous_word_idx = word_idx\n",
        "    labels.append(label_ids)\n",
        "  tokenized_inputs[\"labels\"] = labels\n",
        "  return tokenized_inputs"
      ],
      "metadata": {
        "id": "sfbqDg0o8wIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークン化を反復\n",
        "def encode_panx_dataset(corpus):\n",
        "  return corpus.map(tokenize_and_align_labels, batched=True,\n",
        "                    remove_columns=['langs', 'ner_tags', 'tokens'])"
      ],
      "metadata": {
        "id": "QqaUrnoI9ed4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ドイツ語コーパスをエンコード\n",
        "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])\n",
        "print(panx_de_encoded[\"train\"][0]) # input_ids, attention_mask, labels のリスト"
      ],
      "metadata": {
        "id": "iLjZEtjw9xxD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOWCrsZY90E7Efkz7h/NA+9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWKLD_LckQma"
      },
      "outputs": [],
      "source": [
        "# 必要パッケージのインストール\n",
        "!pip install transformers==4.45.2, seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なものを用意\n",
        "## 使用ベースモデル\n",
        "xlmr_model_name = \"xlm-roberta-base\"\n",
        "\n",
        "## データセットロード\n",
        "from collections import defaultdict\n",
        "from datasets import load_dataset, DatasetDict\n",
        "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
        "fracs = [0.629, 0.229, 0.084, 0.059]\n",
        "panx_ch = defaultdict(DatasetDict)\n",
        "for lang, frac in zip(langs,fracs):\n",
        "  ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
        "  for split in ds:\n",
        "    panx_ch[lang][split] = (\n",
        "        ds[split]\n",
        "        .shuffle(seed=0)\n",
        "        .select(range(int(frac*ds[split].num_rows)))\n",
        "    )\n",
        "\n",
        "## タグの取得\n",
        "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
        "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
        "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n",
        "\n",
        "## データセットテキストのトークン化(エンコード)\n",
        "from transformers import AutoTokenizer\n",
        "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)\n",
        "def tokenize_and_align_labels(examples):\n",
        "  tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\n",
        "                                    is_split_into_words=True)\n",
        "  labels = []\n",
        "  for idx, label in enumerate(examples[\"ner_tags\"]):\n",
        "    word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "    for word_idx in word_ids:\n",
        "      if word_idx is None or word_idx == previous_word_idx:\n",
        "        label_ids.append(-100)\n",
        "      else:\n",
        "        label_ids.append(label[word_idx])\n",
        "      previous_word_idx = word_idx\n",
        "    labels.append(label_ids)\n",
        "  tokenized_inputs[\"labels\"] = labels\n",
        "  return tokenized_inputs\n",
        "def encode_panx_dataset(corpus):\n",
        "  return corpus.map(tokenize_and_align_labels, batched=True,\n",
        "                    remove_columns=['langs', 'ner_tags', 'tokens'])\n",
        "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"]) # エンコード済みドイツ語コーパス\n",
        "\n",
        "## カスタムモデルクラス\n",
        "import torch.nn as nn\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel\n",
        "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "  config_class = XLMRobertaConfig\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.num_labels = config.num_labels\n",
        "    self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "    self.init_weights()\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
        "              labels=None, **kwargs):\n",
        "    outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
        "                           token_type_ids=token_type_ids, **kwargs)\n",
        "    sequence_output = self.dropout(outputs[0])\n",
        "    logits = self.classifier(sequence_output)\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss()\n",
        "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    return TokenClassifierOutput(loss=loss, logits=logits,\n",
        "                                 hidden_states=outputs.hidden_states,\n",
        "                                 attentions=outputs.attentions)\n",
        "\n",
        "## カスタムモデル設定\n",
        "from transformers import AutoConfig\n",
        "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
        "                                         num_labels=tags.num_classes,\n",
        "                                         id2label=index2tag, label2id=tag2index)\n",
        "\n",
        "## データコレーター\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)\n",
        "\n",
        "## モデルロード\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = XLMRobertaForTokenClassification.from_pretrained(\"kirapika2/xlm-roberta-base-finetuned-panx-de\",\n",
        "                                                          config=xlmr_config).to(device)\n",
        "\n",
        "## モデル初期化関数\n",
        "def model_init():\n",
        "  return (XLMRobertaForTokenClassification\n",
        "          .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
        "          .to(device))\n",
        "\n",
        "## Trainer 設定\n",
        "from transformers import TrainingArguments\n",
        "num_epochs = 3\n",
        "batch_size = 24\n",
        "logging_steps = len(panx_de_encoded[\"train\"]) // batch_size\n",
        "model_name = f\"{xlmr_model_name}-finetuned-panx-de\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size, eval_strategy=\"epoch\",\n",
        "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n",
        "    logging_steps=logging_steps, push_to_hub=False\n",
        ")\n",
        "\n",
        "## モデル出力を評価関数に入力できるよう整形\n",
        "import numpy as np\n",
        "def align_predictions(predictions, label_ids):\n",
        "  preds = np.argmax(predictions, axis=2)\n",
        "  batch_size, seq_len = preds.shape\n",
        "  labels_list, preds_list = [], []\n",
        "  for batch_idx in range(batch_size):\n",
        "    example_labels, example_preds = [], []\n",
        "    for seq_idx in range(seq_len):\n",
        "      if label_ids[batch_idx, seq_idx] != -100:\n",
        "        example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
        "        example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
        "    labels_list.append(example_labels)\n",
        "    preds_list.append(example_preds)\n",
        "  return preds_list, labels_list\n",
        "## 予測とラベル抽出\n",
        "from seqeval.metrics import f1_score\n",
        "def compute_metrics(eval_pred):\n",
        "  y_pred, y_true = align_predictions(eval_pred.predictions,\n",
        "                                     eval_pred.label_ids)\n",
        "  return {\"f1\": f1_score(y_true, y_pred)}\n",
        "\n",
        "## Trainer インスタンス作成\n",
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=xlmr_tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=panx_de_encoded[\"train\"],\n",
        "    eval_dataset=panx_de_encoded[\"validation\"]\n",
        ")\n",
        "\n",
        "## 例文での固有表現推定関数\n",
        "import pandas as pd\n",
        "def tag_text(text, tags, model, tokenizer):\n",
        "  tokens = tokenizer(text).tokens()\n",
        "  input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
        "  outputs = model(input_ids)[0]\n",
        "  predictions = torch.argmax(outputs, dim=2)\n",
        "  preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
        "  return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
      ],
      "metadata": {
        "id": "3ZIeazLCk8s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 他言語での評価関数\n",
        "def get_f1_score(trainer, dataset):\n",
        "  return trainer.predict(dataset).metrics[\"test_f1\"]"
      ],
      "metadata": {
        "id": "-2vGXozqnNAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 評価データセットでの性能確認\n",
        "f1_scores = defaultdict(dict)\n",
        "f1_scores[\"de\"][\"de\"] = get_f1_score(trainer, panx_de_encoded[\"test\"])\n",
        "print(f\"F1-score of [de] model on [de] dataset: {f1_scores['de']['de']:.3f}\")"
      ],
      "metadata": {
        "id": "7CRJgVM4nZq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# フランス語での性能確認\n",
        "text_fr = \"Jeff Dean est informaticien chez Google en Californie\"\n",
        "tag_text(text_fr, tags, trainer.model, xlmr_tokenizer)"
      ],
      "metadata": {
        "id": "Vsg1u06_rW87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 各言語での評価関数\n",
        "def evaluate_lang_performance(lang, trainer):\n",
        "  panx_ds = encode_panx_dataset(panx_ch[lang])\n",
        "  return get_f1_score(trainer, panx_ds[\"test\"])\n",
        "\n",
        "# フランス語データセットでの性能確認\n",
        "f1_scores[\"de\"][\"fr\"] = evaluate_lang_performance(\"fr\", trainer)\n",
        "print(f\"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")"
      ],
      "metadata": {
        "id": "BYvHZLoHtx0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# イタリア語での性能確認\n",
        "f1_scores[\"de\"][\"it\"] = evaluate_lang_performance(\"it\", trainer)\n",
        "print(f\"F1-score of [de] model on [it] dataset: {f1_scores['de']['it']:.3f}\")"
      ],
      "metadata": {
        "id": "uPXqznVnuvRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 英語での性能確認\n",
        "f1_scores[\"de\"][\"en\"] = evaluate_lang_performance(\"en\", trainer)\n",
        "print(f\"F1-score of [de] model on [en] dataset: {f1_scores['de']['en']:.3f}\")"
      ],
      "metadata": {
        "id": "ga6iyQXQvG7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 各言語データセットで学習し、F1スコア出力\n",
        "def train_on_subset(dataset, num_samples):\n",
        "  train_ds = dataset[\"train\"].shuffle(seed=42).select(range(num_samples))\n",
        "  valid_ds = dataset[\"validation\"]\n",
        "  test_ds = dataset[\"test\"]\n",
        "  training_args.logging_steps = len(train_ds) // batch_size\n",
        "  trainer = Trainer(model_init=model_init, args=training_args,\n",
        "                    data_collator=data_collator, compute_metrics=compute_metrics,\n",
        "                    train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)\n",
        "  trainer.train()\n",
        "  f1_score = get_f1_score(trainer, test_ds)\n",
        "  return pd.DataFrame.from_dict(\n",
        "      {\"num_samples\": [len(train_ds)], \"f1_score\": [f1_score]}\n",
        "  )"
      ],
      "metadata": {
        "id": "KmGEATe4llKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# フランス語コーパスを各ラベルをIDにエンコード\n",
        "panx_fr_encoded = encode_panx_dataset(panx_ch[\"fr\"])"
      ],
      "metadata": {
        "id": "a-EspgTpnLdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 250事例のデータセットで動作確認\n",
        "metrics_df = train_on_subset(panx_fr_encoded, 250)\n",
        "metrics_df"
      ],
      "metadata": {
        "id": "eEJ148vLnWGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 事例を増やした場合\n",
        "for num_samples in [500, 1000, 2000, 4000]:\n",
        "  new_row_df = train_on_subset(panx_fr_encoded, num_samples)\n",
        "  metrics_df = pd.concat([metrics_df, new_row_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "L5988SxKoGW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 各事例数のスコアをプロット\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "ax.axhline(f1_scores[\"de\"][\"fr\"], ls=\"--\", color=\"r\")\n",
        "metrics_df.set_index(\"num_samples\").plot(ax=ax)\n",
        "plt.legend([\"Zero-shot from de\", \"Fine-tuned on fr\"], loc=\"lower right\")\n",
        "plt.ylim((0, 1))\n",
        "plt.xlabel(\"Number of Training Samples\")\n",
        "plt.ylabel(\"F1 Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YEUWOJyOqzGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ドイツ語とフランス語のコーパスを連結\n",
        "from datasets import concatenate_datasets\n",
        "\n",
        "def concatenate_splits(corpora):\n",
        "  multi_corpus = DatasetDict()\n",
        "  for split in corpora[0].keys():\n",
        "    multi_corpus[split] = concatenate_datasets(\n",
        "        [corpus[split] for corpus in corpora]).shuffle(seed=42)\n",
        "  return multi_corpus\n",
        "\n",
        "## 連結データセット\n",
        "panx_de_fr_encoded = concatenate_splits([panx_de_encoded, panx_fr_encoded])"
      ],
      "metadata": {
        "id": "_EesYlUQsWTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習\n",
        "training_args.logging_steps = len(panx_de_fr_encoded[\"train\"]) // batch_size\n",
        "training_args.output_dir = \"xlm-roberta-base-finetuned-panx-de-fr\"\n",
        "\n",
        "trainer = Trainer(model_init=model_init, args=training_args,\n",
        "                  data_collator=data_collator, compute_metrics=compute_metrics,\n",
        "                  tokenizer=xlmr_tokenizer, train_dataset=panx_de_fr_encoded[\"train\"],\n",
        "                  eval_dataset=panx_de_fr_encoded[\"validation\"])\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "HeYHfsuVtHzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 各言語の評価データセットでの性能確認\n",
        "for lang in langs:\n",
        "  f1 = evaluate_lang_performance(lang, trainer)\n",
        "  print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f}\")"
      ],
      "metadata": {
        "id": "q0jCsRqhuChm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 各言語コーパスでファインチューニング\n",
        "corpora = [panx_de_encoded]\n",
        "\n",
        "# ドイツ語以外に対してforを回す\n",
        "for lang in langs[1:]:\n",
        "  training_args.output_dir = f\"xlm-roberta-base-finetuned-panx-{lang}\"\n",
        "  # 単一言語のコーパスでファインチューニング\n",
        "  ds_encoded = encode_panx_dataset(panx_ch[lang])\n",
        "  metrics = train_on_subset(ds_encoded, ds_encoded[\"train\"].num_rows)\n",
        "  # F1スコアを収集\n",
        "  f1_scores[lang][lang] = metrics[\"f1_score\"][0]\n",
        "  # 連結のため、単一言語のコーパスをコーパスリストに追加\n",
        "  corpora.append(ds_encoded)"
      ],
      "metadata": {
        "id": "-LfjoPtCvET-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 全言語コーパス作成\n",
        "corpora_encoded = concatenate_splits(corpora)"
      ],
      "metadata": {
        "id": "DNoRzoOVvxa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## huggingface ログイン\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "979gc6ARzFGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 全言語で学習\n",
        "training_args.logging_steps = len(corpora_encoded[\"train\"]) // batch_size\n",
        "training_args.push_to_hub = True\n",
        "training_args.output_dir = \"xlm-roberta-base-finetuned-panx-all\"\n",
        "\n",
        "trainer = Trainer(model_init=model_init, args=training_args,\n",
        "                  data_collator=data_collator, compute_metrics=compute_metrics,\n",
        "                  tokenizer=xlmr_tokenizer, train_dataset=corpora_encoded[\"train\"],\n",
        "                  eval_dataset=corpora_encoded[\"validation\"])\n",
        "\n",
        "trainer.train()\n",
        "trainer.push_to_hub(commit_message=\"Training completed!\")"
      ],
      "metadata": {
        "id": "6JlifeG5v4Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 各ファインチューニングの結果比較\n",
        "## 各言語の評価データセットで全言語学習モデルの性能評価\n",
        "for idx, lang in enumerate(langs):\n",
        "  f1_scores[\"all\"][lang] = get_f1_score(trainer, corpora[idx][\"test\"])\n",
        "\n",
        "## 集計\n",
        "scores_data = {\"de\": f1_scores[\"de\"],\n",
        "               \"each\": {lang: f1_scores[lang][lang] for lang in langs},\n",
        "               \"all\": f1_scores[\"all\"]}\n",
        "f1_scores_df = pd.DataFrame(scores_data).T.round(4)\n",
        "f1_scores_df.rename_axis(index=\"Fine-tune on\", columns=\"Evaluated on\",\n",
        "                         inplace=True)\n",
        "f1_scores_df"
      ],
      "metadata": {
        "id": "IwjZH5lowa1_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
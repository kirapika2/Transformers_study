{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UaBnknFk9Nv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# 必要パッケージのインストール\n",
        "## v5.0.0 では Jupyter でカスタムモデルのロードができないため、<=4.52.4(推定) が必要\n",
        "## 該当 Issue: https://github.com/huggingface/transformers/issues/43645\n",
        "## 一方、BERT 系のモデルでは forward に失敗する問題があり、<=4.45.2(推定), >=4.57.0 が必要\n",
        "## 該当 Issue: https://github.com/huggingface/transformers/issues/35838\n",
        "## → 5.1.0 では要件を満たすが、仕様が新しいことで別種の問題も生じるため、古いバージョンでの実行とする\n",
        "!pip install transformers==4.45.2, seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用ベースモデル\n",
        "xlmr_model_name = \"xlm-roberta-base\""
      ],
      "metadata": {
        "id": "3ViZzoQBmipY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットロード\n",
        "from collections import defaultdict\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "## 話者比率でロード\n",
        "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
        "fracs = [0.629, 0.229, 0.084, 0.059]\n",
        "panx_ch = defaultdict(DatasetDict)\n",
        "for lang, frac in zip(langs,fracs):\n",
        "  ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
        "  for split in ds:\n",
        "    panx_ch[lang][split] = (\n",
        "        ds[split]\n",
        "        .shuffle(seed=0)\n",
        "        .select(range(int(frac*ds[split].num_rows)))\n",
        "    )\n",
        "\n",
        "## タグの取得\n",
        "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
        "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
        "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BN4ZT2hLl_Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットテキストのトークン化(エンコード)\n",
        "from transformers import AutoTokenizer\n",
        "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "  tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\n",
        "                                    is_split_into_words=True)\n",
        "  labels = []\n",
        "  for idx, label in enumerate(examples[\"ner_tags\"]):\n",
        "    word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "    for word_idx in word_ids:\n",
        "      if word_idx is None or word_idx == previous_word_idx:\n",
        "        label_ids.append(-100)\n",
        "      else:\n",
        "        label_ids.append(label[word_idx])\n",
        "      previous_word_idx = word_idx\n",
        "    labels.append(label_ids)\n",
        "  tokenized_inputs[\"labels\"] = labels\n",
        "  return tokenized_inputs\n",
        "\n",
        "def encode_panx_dataset(corpus):\n",
        "  return corpus.map(tokenize_and_align_labels, batched=True,\n",
        "                    remove_columns=['langs', 'ner_tags', 'tokens'])\n",
        "\n",
        "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"]) # エンコード済みドイツ語コーパス"
      ],
      "metadata": {
        "collapsed": true,
        "id": "R-Z_CeAbma1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# カスタムモデル\n",
        "import torch.nn as nn\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel\n",
        "\n",
        "## カスタムモデルクラス\n",
        "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "  config_class = XLMRobertaConfig\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.num_labels = config.num_labels\n",
        "    # ボディをロード\n",
        "    self.roberta = RobertaModel(config, add_pooling_layer=False) # [CLS]トークンによる表現抽出層の無効化\n",
        "    # トークン分類ヘッドの用意\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "    # 重みのロードと初期化\n",
        "    self.init_weights()\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
        "              labels=None, **kwargs):\n",
        "    # ボディによりエンコーダの表現を取得\n",
        "    outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
        "                           token_type_ids=token_type_ids, **kwargs)\n",
        "    # 分類器を適用\n",
        "    sequence_output = self.dropout(outputs[0]) # 最後の隠れ状態\n",
        "    logits = self.classifier(sequence_output)\n",
        "\n",
        "    # 損失計算\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss()\n",
        "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    # モデルの出力オブジェクトとして返す\n",
        "    return TokenClassifierOutput(loss=loss, logits=logits,\n",
        "                                 hidden_states=outputs.hidden_states,\n",
        "                                 attentions=outputs.attentions)\n",
        "\n",
        "## カスタムモデル設定\n",
        "from transformers import AutoConfig\n",
        "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
        "                                         num_labels=tags.num_classes,\n",
        "                                         id2label=index2tag, label2id=tag2index)"
      ],
      "metadata": {
        "id": "flU8wZ8nn25p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習パラメータ定義\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "num_epochs = 3\n",
        "batch_size = 24\n",
        "logging_steps = len(panx_de_encoded[\"train\"]) // batch_size # 各バッチごとにログを出す\n",
        "model_name = f\"{xlmr_model_name}-finetuned-panx-de\"\n",
        "# モデル設定\n",
        "## evalutation_strategy ではなく eval_strategy になっている\n",
        "## 参考 Issue: https://github.com/huggingface/setfit/issues/512\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size, eval_strategy=\"epoch\",\n",
        "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n",
        "    logging_steps=logging_steps, push_to_hub=True\n",
        ")"
      ],
      "metadata": {
        "id": "20z5BvBKoO-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# huggingface ログイン\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "WN1QFN4OrPVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 予測とラベルの出力\n",
        "import numpy as np\n",
        "\n",
        "## モデル出力を評価関数に入力できるよう整形\n",
        "def align_predictions(predictions, label_ids):\n",
        "  preds = np.argmax(predictions, axis=2)\n",
        "  batch_size, seq_len = preds.shape\n",
        "  labels_list, preds_list = [], []\n",
        "\n",
        "  for batch_idx in range(batch_size):\n",
        "    example_labels, example_preds = [], []\n",
        "    for seq_idx in range(seq_len):\n",
        "      if label_ids[batch_idx, seq_idx] != -100:\n",
        "        example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
        "        example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
        "    labels_list.append(example_labels)\n",
        "    preds_list.append(example_preds)\n",
        "\n",
        "  return preds_list, labels_list\n",
        "\n",
        "## 予測とラベル抽出\n",
        "from seqeval.metrics import f1_score\n",
        "def compute_metrics(eval_pred):\n",
        "  y_pred, y_true = align_predictions(eval_pred.predictions,\n",
        "                                     eval_pred.label_ids)\n",
        "  return {\"f1\": f1_score(y_true, y_pred)}"
      ],
      "metadata": {
        "id": "hbJQJ9pzrgA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データコレーター\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)"
      ],
      "metadata": {
        "id": "uvMrcJQSsPff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル初期化\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def model_init():\n",
        "  return (XLMRobertaForTokenClassification\n",
        "          .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
        "          .to(device))"
      ],
      "metadata": {
        "id": "8lssrOkNtBkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer 用意\n",
        "from transformers import Trainer\n",
        "trainer = Trainer(model_init=model_init, args=training_args,\n",
        "                  data_collator=data_collator, compute_metrics=compute_metrics,\n",
        "                  train_dataset=panx_de_encoded[\"train\"],\n",
        "                  eval_dataset=panx_de_encoded[\"validation\"],\n",
        "                  tokenizer=xlmr_tokenizer)"
      ],
      "metadata": {
        "id": "iZMkqTxZt47P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習\n",
        "trainer.train()\n",
        "trainer.push_to_hub(commit_message=\"Training completed!\")"
      ],
      "metadata": {
        "id": "kArw-08OvRtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ドイツ語の例文でテスト\n",
        "import pandas as pd\n",
        "def tag_text(text, tags, model, tokenizer):\n",
        "  tokens = tokenizer(text).tokens()\n",
        "  input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
        "  outputs = model(input_ids)[0]\n",
        "  predictions = torch.argmax(outputs, dim=2)\n",
        "  preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
        "  return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n",
        "\n",
        "text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\n",
        "tag_text(text_de, tags, trainer.model, xlmr_tokenizer)"
      ],
      "metadata": {
        "id": "VKDNcn0419KE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMWYl0UvU7GtfBuAe8gjKHc"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
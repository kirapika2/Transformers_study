{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLULbTO8f7EQBUnSmqfStS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# エラーの出ないバージョン\n",
        "!pip install transformers==4.45.2"
      ],
      "metadata": {
        "id": "lxsWq1xbB_3g",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なものを用意\n",
        "## 使用ベースモデル\n",
        "xlmr_model_name = \"xlm-roberta-base\"\n",
        "\n",
        "## データセットロード\n",
        "from collections import defaultdict\n",
        "from datasets import load_dataset, DatasetDict\n",
        "## 話者比率でロード\n",
        "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
        "fracs = [0.629, 0.229, 0.084, 0.059]\n",
        "panx_ch = defaultdict(DatasetDict)\n",
        "for lang, frac in zip(langs,fracs):\n",
        "  ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
        "  for split in ds:\n",
        "    panx_ch[lang][split] = (\n",
        "        ds[split]\n",
        "        .shuffle(seed=0)\n",
        "        .select(range(int(frac*ds[split].num_rows)))\n",
        "    )\n",
        "\n",
        "## タグの取得\n",
        "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
        "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
        "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n",
        "\n",
        "## データセットテキストのトークン化(エンコード)\n",
        "from transformers import AutoTokenizer\n",
        "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "  tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\n",
        "                                    is_split_into_words=True)\n",
        "  labels = []\n",
        "  for idx, label in enumerate(examples[\"ner_tags\"]):\n",
        "    word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "    for word_idx in word_ids:\n",
        "      if word_idx is None or word_idx == previous_word_idx:\n",
        "        label_ids.append(-100)\n",
        "      else:\n",
        "        label_ids.append(label[word_idx])\n",
        "      previous_word_idx = word_idx\n",
        "    labels.append(label_ids)\n",
        "  tokenized_inputs[\"labels\"] = labels\n",
        "  return tokenized_inputs\n",
        "\n",
        "def encode_panx_dataset(corpus):\n",
        "  return corpus.map(tokenize_and_align_labels, batched=True,\n",
        "                    remove_columns=['langs', 'ner_tags', 'tokens'])\n",
        "\n",
        "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"]) # エンコード済みドイツ語コーパス\n",
        "\n",
        "## カスタムモデルクラス\n",
        "import torch.nn as nn\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel\n",
        "\n",
        "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "  config_class = XLMRobertaConfig\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.num_labels = config.num_labels\n",
        "    # ボディをロード\n",
        "    self.roberta = RobertaModel(config, add_pooling_layer=False) # [CLS]トークンによる表現抽出層の無効化\n",
        "    # トークン分類ヘッドの用意\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "    # 重みのロードと初期化\n",
        "    self.init_weights()\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
        "              labels=None, **kwargs):\n",
        "    # ボディによりエンコーダの表現を取得\n",
        "    outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
        "                           token_type_ids=token_type_ids, **kwargs)\n",
        "    # 分類器を適用\n",
        "    sequence_output = self.dropout(outputs[0]) # 最後の隠れ状態\n",
        "    logits = self.classifier(sequence_output)\n",
        "\n",
        "    # 損失計算\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss()\n",
        "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    # モデルの出力オブジェクトとして返す\n",
        "    return TokenClassifierOutput(loss=loss, logits=logits,\n",
        "                                 hidden_states=outputs.hidden_states,\n",
        "                                 attentions=outputs.attentions)\n",
        "\n",
        "## カスタムモデル設定\n",
        "from transformers import AutoConfig\n",
        "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
        "                                         num_labels=tags.num_classes,\n",
        "                                         id2label=index2tag, label2id=tag2index)\n",
        "\n",
        "## データコレーター\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)\n",
        "\n",
        "## モデルロード\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = XLMRobertaForTokenClassification.from_pretrained(\"kirapika2/xlm-roberta-base-finetuned-panx-de\",\n",
        "                                                          config=xlmr_config).to(device)"
      ],
      "metadata": {
        "id": "CSW-stew_sp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsSZXVKB9-lr"
      },
      "outputs": [],
      "source": [
        "# トークンごとの損失計算\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "def forward_pass_with_label(batch):\n",
        "  # リストの辞書をデータコレーターに適した、辞書のリストに変換\n",
        "  features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
        "  # 入力ラベルをパディングし、全てのテンソルをデバイスにのせる\n",
        "  batch = data_collator(features)\n",
        "  input_ids = batch[\"input_ids\"].to(device)\n",
        "  attention_mask = batch[\"attention_mask\"].to(device)\n",
        "  labels = batch[\"labels\"].to(device)\n",
        "  with torch.no_grad():\n",
        "    # データをモデルに渡す\n",
        "    output = model(input_ids, attention_mask)\n",
        "    # logit.size: [batch_size, sequence_length, classes]\n",
        "    # ロジットが最大のクラスを予測\n",
        "    predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
        "  # view を使ってバッチ次元をFlattenし、トークンごとの損失を計算\n",
        "  loss = cross_entropy(output.logits.view(-1, 7),\n",
        "                       labels.view(-1), reduction=\"none\")\n",
        "  # バッチ次元をUnflattenし、Numpy配列に変換\n",
        "  loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
        "\n",
        "  return {\"loss\": loss, \"predicted_label\": predicted_label}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 検証データセット全体の損失計算\n",
        "valid_set = panx_de_encoded[\"validation\"]\n",
        "valid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\n",
        "df = valid_set.to_pandas()"
      ],
      "metadata": {
        "id": "R8c4lf8FD6jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df の整形\n",
        "index2tag[-100] = \"IGN\"\n",
        "df[\"input_tokens\"] = df[\"input_ids\"].apply(\n",
        "    lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\n",
        "## 各ラベルをIDからタグ名に\n",
        "df[\"predicted_label\"] = df[\"predicted_label\"].apply(\n",
        "    lambda x: [index2tag[i] for i in x])\n",
        "df[\"labels\"] = df[\"labels\"].apply(\n",
        "    lambda x: [index2tag[i] for i in x])\n",
        "## 出力内容は入力サイズに切り詰める\n",
        "df[\"loss\"] = df.apply(\n",
        "    lambda x: x[\"loss\"][:len(x[\"input_ids\"])], axis=1)\n",
        "df[\"predicted_label\"] = df.apply(\n",
        "    lambda x: x[\"predicted_label\"][:len(x[\"input_ids\"])], axis=1)\n",
        "## 内容確認\n",
        "df.head(1)"
      ],
      "metadata": {
        "id": "GB63Ne9zMp0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークンごとの結果に整形\n",
        "import pandas as pd\n",
        "df_tokens = df.apply(pd.Series.explode)\n",
        "df_tokens = df_tokens.query(\"labels != 'IGN'\")\n",
        "df_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\n",
        "df_tokens.head(7)"
      ],
      "metadata": {
        "id": "WmUJzrcSPrDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 損失の多いトークンを特定\n",
        "(\n",
        "    df_tokens.groupby(\"input_tokens\")['loss']\n",
        "    .agg([\"count\", \"mean\", \"sum\"])\n",
        "    .sort_values(by=\"sum\", ascending=False)\n",
        "    .reset_index()\n",
        "    .round(2)\n",
        "    .head(10)\n",
        "    .T\n",
        ")"
      ],
      "metadata": {
        "id": "uMWOuT-DQgAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 損失の多いラベルを特定\n",
        "(\n",
        "    df_tokens.groupby(\"labels\")['loss']\n",
        "    .agg([\"count\", \"mean\", \"sum\"])\n",
        "    .sort_values(by=\"mean\", ascending=False)\n",
        "    .reset_index()\n",
        "    .round(2)\n",
        "    .T\n",
        ")"
      ],
      "metadata": {
        "id": "tmSjY8SWTxHg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
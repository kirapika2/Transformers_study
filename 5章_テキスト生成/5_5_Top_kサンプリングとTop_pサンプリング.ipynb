{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNveFbCoNwLiCmXLRiNsvw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 必要なものを用意\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"gpt2-xl\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "max_length = 128\n",
        "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
        "a herd of unicorns living in a remote, previously unexplored \\\n",
        "valley, in the Andes Mountains. Even more surprising to the \\\n",
        "researchers was the fact that the unicorns spoke perfect English.\\n\\n\"\"\"\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)"
      ],
      "metadata": {
        "id": "IFWJz8xsVP6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# matplotlib での日本語表示\n",
        "## 参考：https://qiita.com/naohiro2g/items/5cb79763a14e052db768\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib"
      ],
      "metadata": {
        "id": "ntUpOqZFS99L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPIe-OeTLjan"
      },
      "outputs": [],
      "source": [
        "# copilot 製の確率可視化コード\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# ===== 設定 =====\n",
        "ja_model_name = \"rinna/japanese-gpt2-small\"\n",
        "prompt = \"今日の天気は\"\n",
        "\n",
        "temperature = 1.0\n",
        "min_p, max_p = 1e-10, 1e-1   # 確率分布(対数)の表示範囲\n",
        "num_bins = 80\n",
        "\n",
        "k_threshold = 2000           # Top-k 閾値\n",
        "p_threshold = 0.95           # Top-p 閾値\n",
        "xmax_cdf = 10000             # 累積分布の横軸上限\n",
        "\n",
        "# ===== モデル読み込み =====\n",
        "ja_tokenizer = AutoTokenizer.from_pretrained(ja_model_name)\n",
        "ja_model = AutoModelForCausalLM.from_pretrained(ja_model_name)\n",
        "ja_model.eval()\n",
        "\n",
        "inputs = ja_tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# ===== 次トークン確率（T=1） =====\n",
        "with torch.no_grad():\n",
        "    outputs = ja_model(**inputs)\n",
        "    logits = outputs.logits[0, -1, :]\n",
        "    probs = torch.softmax(logits / temperature, dim=-1).cpu().numpy()\n",
        "\n",
        "# ===== 1) 確率分布（対数ヒストグラム） =====\n",
        "plot_probs = probs[(probs >= min_p) & (probs <= max_p)]\n",
        "log_bins = np.logspace(np.log10(min_p), np.log10(max_p), num_bins)\n",
        "counts, edges = np.histogram(plot_probs, bins=log_bins)\n",
        "\n",
        "# ===== 2) 累積分布（高確率順） =====\n",
        "sorted_probs = np.sort(probs)[::-1]\n",
        "cum_probs = np.cumsum(sorted_probs)\n",
        "ranks = np.arange(1, len(sorted_probs) + 1)\n",
        "\n",
        "# ===== 描画 =====\n",
        "fig, ax = plt.subplots(1, 2, figsize=(13, 4.5))\n",
        "\n",
        "# 左: 確率分布\n",
        "ax[0].bar(edges[:-1], counts, width=np.diff(edges), align=\"edge\")\n",
        "ax[0].set_xscale(\"log\")\n",
        "ax[0].set_yscale(\"log\")\n",
        "ax[0].set_xlim(min_p, max_p)\n",
        "ax[0].set_xlabel(\"確率\")\n",
        "ax[0].set_ylabel(\"カウント\")\n",
        "ax[0].set_title(\"確率分布\")\n",
        "\n",
        "# 右: 累積分布\n",
        "ax[1].plot(ranks, cum_probs, lw=1.5)\n",
        "ax[1].set_xlim(0, xmax_cdf)\n",
        "ax[1].set_ylim(0, 1.01)\n",
        "ax[1].set_xlabel(\"トークン(確率の高い順)\")\n",
        "ax[1].set_ylabel(\"確率\")\n",
        "ax[1].set_title(\"累積分布\")\n",
        "\n",
        "# 閾値線\n",
        "ax[1].axvline(k_threshold, color=\"tab:orange\", ls=\"-\", lw=1.5, label=f\"Top-k threshold: k={k_threshold}\")\n",
        "ax[1].axhline(p_threshold, color=\"tab:red\", ls=\"--\", lw=1.5, label=f\"Top-p threshold: p={p_threshold}\")\n",
        "ax[1].legend(loc=\"lower right\")\n",
        "\n",
        "japanize_matplotlib.japanize()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-k サンプリング\n",
        "output_topk = model.generate(input_ids, max_length=max_length, do_sample=True,\n",
        "                             top_k=50)\n",
        "print(tokenizer.decode(output_topk[0]))"
      ],
      "metadata": {
        "id": "FHbyN1Z1ZIM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-p サンプリング\n",
        "output_topp = model.generate(input_ids, max_length=max_length, do_sample=True,\n",
        "                             top_p=0.90)\n",
        "print(tokenizer.decode(output_topp[0]))"
      ],
      "metadata": {
        "id": "qNkXgcagbwIl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdo39HuiQuhVxhkj3bMCC3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78Sy0JjqC_Hk"
      },
      "outputs": [],
      "source": [
        "# 文字トークン化\n",
        "text = \"Tokenizing text is a core task of NLP.\"\n",
        "tokenized_text = list(text)\n",
        "print(tokenized_text) # スペース含めた1文字ずつのリスト"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# char2int 辞書作成\n",
        "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))} # ASCII 順に各文字にid(数値)をあてる\n",
        "print(token2idx)"
      ],
      "metadata": {
        "id": "_tLP48J3Edqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークン化\n",
        "input_ids = [token2idx[token] for token in tokenized_text]\n",
        "print(input_ids) # 入力ベクトル"
      ],
      "metadata": {
        "id": "JB2zV1RqFibq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot ベクトル化例\n",
        "import pandas as pd\n",
        "\n",
        "categorical_df = pd.DataFrame({\"Name\": [\"島村卯月\", \"渋谷凛\", \"本田未央\"], \"Label ID\": [0, 1, 2]}) # 名義尺度として扱いたい\n",
        "categorical_df\n",
        "pd.get_dummies(categorical_df[\"Name\"]) # 各行を one-hot ベクトル化"
      ],
      "metadata": {
        "id": "AIrWl2vXIQxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot エンコーディング\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "one_hot_encodings = F.one_hot(torch.tensor(input_ids), num_classes=len(token2idx))\n",
        "one_hot_encodings.shape"
      ],
      "metadata": {
        "id": "aQAKXMHQKX30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T(1文字目) のベクトル\n",
        "print(f\"Token: {tokenized_text[0]}\")\n",
        "print(f\"Tensor index: {input_ids[0]}\")\n",
        "print(f\"One-hot: {one_hot_encodings[0]}\")"
      ],
      "metadata": {
        "id": "d9K2x6d3Ly3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 単語トークン化\n",
        "tokenized_text = text.split()\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "id": "96mWglNpswUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークナイザロード\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ckpt = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "PUgRG4RJvQk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBERT トークナイザ\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "EbJlGC8HwLlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# エンコード\n",
        "encoded_text = tokenizer(text)\n",
        "print(encoded_text)"
      ],
      "metadata": {
        "id": "KJldokqDxQPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークン内容\n",
        "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "OIFigYfZxXvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークンの文字列化\n",
        "print(tokenizer.convert_tokens_to_string(tokens))"
      ],
      "metadata": {
        "id": "8IOL-ickyV35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークナイザーの情報\n",
        "print(tokenizer.vocab_size)\n",
        "print(tokenizer.model_max_length)\n",
        "print(tokenizer.model_input_names)"
      ],
      "metadata": {
        "id": "KHPjwHjSy-s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークン化してエンコードする関数\n",
        "def tokenize(batch):\n",
        "  return tokenizer(batch[\"text\"], padding=True, truncation=True)"
      ],
      "metadata": {
        "id": "LyRA7nSmz6hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットのバッチ処理\n",
        "## データセット\n",
        "from datasets import load_dataset\n",
        "emotions = load_dataset(\"dair-ai/emotion\")\n",
        "\n",
        "## 2行分エンコード\n",
        "print(tokenize(emotions[\"train\"][:2])) # input_ids、attention_mask が2つのリストが入ったリストになり、片方はパディングされている"
      ],
      "metadata": {
        "id": "n4b80zEd0XlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 全部エンコード\n",
        "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
        "\n",
        "## 列が追加される\n",
        "print(emotions_encoded[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "URClJf7D2S1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}